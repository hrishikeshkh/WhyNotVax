{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SC making the NN a pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sc Vax Classify\n"
     ]
    }
   ],
   "source": [
    "print(\"Sc Vax Classify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1296010336907038720t</td>\n",
       "      <td>@cath__kath AstraZeneca is made with the kidne...</td>\n",
       "      <td>ingredients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1336808189677940736t</td>\n",
       "      <td>It begins. Please find safe alternatives to th...</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1329488407307956231t</td>\n",
       "      <td>@PaolaQP1231 Well, I mean congratulations Covi...</td>\n",
       "      <td>side-effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1364194604459900934t</td>\n",
       "      <td>@BorisJohnson for those of us that do not wish...</td>\n",
       "      <td>mandatory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1375938799247765515t</td>\n",
       "      <td>She has been trying to speak out: writing lett...</td>\n",
       "      <td>side-effect rushed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                                              tweet  \\\n",
       "0  1296010336907038720t  @cath__kath AstraZeneca is made with the kidne...   \n",
       "1  1336808189677940736t  It begins. Please find safe alternatives to th...   \n",
       "2  1329488407307956231t  @PaolaQP1231 Well, I mean congratulations Covi...   \n",
       "3  1364194604459900934t  @BorisJohnson for those of us that do not wish...   \n",
       "4  1375938799247765515t  She has been trying to speak out: writing lett...   \n",
       "\n",
       "               labels  \n",
       "0         ingredients  \n",
       "1         side-effect  \n",
       "2         side-effect  \n",
       "3           mandatory  \n",
       "4  side-effect rushed  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data\n",
    "df = pd.read_csv(\"data/train_val.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std defn\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for each class\n",
    "attr = [\n",
    "    \"unnecessary\",\n",
    "    \"mandatory\",\n",
    "    \"pharma\",\n",
    "    \"conspiracy\",\n",
    "    \"political\",\n",
    "    \"country\",\n",
    "    \"rushed\",\n",
    "    \"ingredients\",\n",
    "    \"side-effect\",\n",
    "    \"ineffective\",\n",
    "    \"religious\",\n",
    "    \"none\",\n",
    "]\n",
    "\n",
    "accuracies = {}\n",
    "# split data for each class\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'unnecessary', set the label columnn to 1 else 0\n",
    "# make a copy of the df and set it to another variable\n",
    "df_unnecessary = df.copy()\n",
    "df_unnecessary[\"unnecessary\"] = df_unnecessary[\"labels\"].apply(\n",
    "    lambda x: 1 if \"unnecessary\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'mandatory', set the label columnn to 1 else 0\n",
    "df_mandatory = df.copy()\n",
    "df_mandatory[\"mandatory\"] = df_mandatory[\"labels\"].apply(\n",
    "    lambda x: 1 if \"mandatory\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'pharma', set the label columnn to 1 else 0\n",
    "df_pharma = df.copy()\n",
    "df_pharma[\"pharma\"] = df_pharma[\"labels\"].apply(lambda x: 1 if \"pharma\" in x else 0)\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'conspiracy', set the label columnn to 1 else 0\n",
    "df_conspiracy = df.copy()\n",
    "df_conspiracy[\"conspiracy\"] = df_conspiracy[\"labels\"].apply(\n",
    "    lambda x: 1 if \"conspiracy\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'political', set the label columnn to 1 else 0\n",
    "df_political = df.copy()\n",
    "df_political[\"political\"] = df_political[\"labels\"].apply(\n",
    "    lambda x: 1 if \"political\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'country', set the label columnn to 1 else 0\n",
    "df_country = df.copy()\n",
    "df_country[\"country\"] = df_country[\"labels\"].apply(lambda x: 1 if \"country\" in x else 0)\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'rushed', set the label columnn to 1 else 0\n",
    "df_rushed = df.copy()\n",
    "df_rushed[\"rushed\"] = df_rushed[\"labels\"].apply(lambda x: 1 if \"rushed\" in x else 0)\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'ingredients', set the label columnn to 1 else 0\n",
    "df_ingredients = df.copy()\n",
    "df_ingredients[\"ingredients\"] = df_ingredients[\"labels\"].apply(\n",
    "    lambda x: 1 if \"ingredients\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'side-effect', set the label columnn to 1 else 0\n",
    "df_side_effect = df.copy()\n",
    "df_side_effect[\"side-effect\"] = df_side_effect[\"labels\"].apply(\n",
    "    lambda x: 1 if \"side-effect\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'ineffective', set the label columnn to 1 else 0\n",
    "df_ineffective = df.copy()\n",
    "df_ineffective[\"ineffective\"] = df_ineffective[\"labels\"].apply(\n",
    "    lambda x: 1 if \"ineffective\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'religious', set the label columnn to 1 else 0\n",
    "df_religious = df.copy()\n",
    "df_religious[\"religious\"] = df_religious[\"labels\"].apply(\n",
    "    lambda x: 1 if \"religious\" in x else 0\n",
    ")\n",
    "\n",
    "# copy the df and modify such that for those rows where the string value of labels column contains 'none', set the label columnn to 1 else 0\n",
    "df_none = df.copy()\n",
    "df_none[\"none\"] = df_none[\"labels\"].apply(lambda x: 1 if \"none\" in x else 0)\n",
    "\n",
    "df_all = [\n",
    "    df_unnecessary,\n",
    "    df_mandatory,\n",
    "    df_pharma,\n",
    "    df_conspiracy,\n",
    "    df_political,\n",
    "    df_country,\n",
    "    df_rushed,\n",
    "    df_ingredients,\n",
    "    df_side_effect,\n",
    "    df_ineffective,\n",
    "    df_religious,\n",
    "    df_none,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    # Vectorization parameters\n",
    "    # Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "    NGRAM_RANGE = (1, 2)\n",
    "\n",
    "    # Limit on the number of features. We use the top 20K features.\n",
    "    TOP_K = 2000\n",
    "\n",
    "    # Whether text should be split into word or character n-grams.\n",
    "    # One of 'word', 'char'.\n",
    "    TOKEN_MODE = \"word\"\n",
    "\n",
    "    # Minimum document/corpus frequency below which a token will be discarded.\n",
    "    MIN_DOCUMENT_FREQUENCY = 2\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "        \"ngram_range\": NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        \"dtype\": np.float64,\n",
    "        \"strip_accents\": \"unicode\",\n",
    "        \"decode_error\": \"replace\",\n",
    "        \"analyzer\": TOKEN_MODE,  # Split text into word tokens.\n",
    "        \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype(\"float32\")\n",
    "    x_val = selector.transform(x_val).astype(\"float32\")\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, x_val, y_val):\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Compute the predictions on the validation set.\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_val)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "    # Compute the accuracy of the model on the validation set.\n",
    "    correct_predictions = torch.eq(predictions, y_val).sum().item()\n",
    "    total_predictions = y_val.shape[0]\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch nn model\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers, units, dropout_rate, input_shape, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(input_shape, units)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(layers - 1):\n",
    "            self.layers.append(nn.Linear(units, units))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "        self.fc2 = nn.Linear(units, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have a bunch of options in front of us: binary chain, ensembling (best but maybe complex to choose b/w ensembling methods), power set (not rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_model(\n",
    "    data,\n",
    "    name,\n",
    "    learning_rate=10e-3,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    layers=2,\n",
    "    units=64,\n",
    "    dropout_rate=0.2,\n",
    "):\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
    "    #print(x_train.shape)\n",
    "    x_train_int = int(x_train.shape[0]), int(x_train.shape[1])\n",
    "    #Create model instance.\n",
    "    model = MLP(\n",
    "        layers=layers,\n",
    "        units=units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        input_shape=x_train_int[1],\n",
    "        num_classes=2,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = \"binary_crossentropy\"\n",
    "    else:\n",
    "        loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    x_train = torch.from_numpy(x_train.toarray()).float()\n",
    "    train_tuple = torch.tensor(train_labels.values)\n",
    "    y_train = F.one_hot(train_tuple, num_classes=2).float()\n",
    "\n",
    "    # Create a TensorDataset from your training data.\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "\n",
    "    # Create a DataLoader from the TensorDataset.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"models/model\" + name +\".pt\"))\n",
    "    except:\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                # Zero the gradients.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass.\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass.\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters.\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print the loss.\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "    first_row = x_val.toarray()[0]\n",
    "\n",
    "    x_val_tensor = torch.from_numpy(x_val.toarray()).float()\n",
    "    y_val_tensor = torch.tensor(val_labels.values)\n",
    "    accuracy = get_accuracy(model, x_val_tensor, y_val_tensor)\n",
    "\n",
    "    # Print the accuracy.\n",
    "    print(\"Validation accuracy:\", accuracy)\n",
    "    accuracies[name] = accuracy\n",
    "    torch.save(model.state_dict(), \"models/model\" + name + \".pt\")\n",
    "\n",
    "    first_row_tensor = torch.tensor(first_row)\n",
    "\n",
    "    train_labels_list = train_labels.tolist()\n",
    "\n",
    "    misses = 0\n",
    "    print(\"training\")\n",
    "    for ind, val in enumerate(train_labels_list):\n",
    "        if True:\n",
    "            first_row = x_val.toarray()[ind]\n",
    "            first_row_tensor = torch.tensor(first_row)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                prediction = model.forward(first_row_tensor)\n",
    "\n",
    "            \n",
    "            prediction = prediction.tolist().index(max(prediction.tolist()))\n",
    "            if prediction != val:\n",
    "                misses += 1\n",
    "\n",
    "            if ind > 1500:\n",
    "                break\n",
    "\n",
    "    print(\"misses\", misses/1500)     \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model.forward(first_row_tensor)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unnecessary\n",
      "Validation accuracy: 0.9299748110831234\n",
      "training\n",
      "misses 0.12\n",
      "mandatory\n",
      "Validation accuracy: 0.9355163727959698\n",
      "training\n",
      "misses 0.11333333333333333\n",
      "pharma\n",
      "Validation accuracy: 0.8846347607052897\n",
      "training\n",
      "misses 0.18466666666666667\n",
      "conspiracy\n",
      "Validation accuracy: 0.947103274559194\n",
      "training\n",
      "misses 0.07066666666666667\n",
      "political\n",
      "Validation accuracy: 0.9355163727959698\n",
      "training\n",
      "misses 0.10266666666666667\n",
      "country\n",
      "Validation accuracy: 0.980352644836272\n",
      "training\n",
      "misses 0.032\n",
      "rushed\n",
      "Validation accuracy: 0.9052896725440807\n",
      "training\n",
      "misses 0.21533333333333332\n",
      "ingredients\n",
      "Validation accuracy: 0.9551637279596977\n",
      "training\n",
      "misses 0.048666666666666664\n",
      "side-effect\n",
      "Validation accuracy: 0.8473551637279597\n",
      "training\n",
      "misses 0.478\n",
      "ineffective\n",
      "Validation accuracy: 0.8639798488664987\n",
      "training\n",
      "misses 0.24\n",
      "religious\n",
      "Validation accuracy: 0.9949622166246851\n",
      "training\n",
      "misses 0.007333333333333333\n",
      "none\n",
      "Validation accuracy: 0.9350125944584383\n",
      "training\n",
      "misses 0.09\n"
     ]
    }
   ],
   "source": [
    "for ind, i in enumerate(df_all):\n",
    "    # Split the data into training and testing sets\n",
    "    print(attr[ind])\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        i.iloc[:, 1], i.iloc[:, -1], test_size=0.2, random_state=42\n",
    "    )\n",
    "    data = (train_texts, train_labels), (val_texts, val_labels)\n",
    "\n",
    "    train_ngram_model(data, name=attr[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unnecessary': 0.9299748110831234, 'mandatory': 0.9355163727959698, 'pharma': 0.8846347607052897, 'conspiracy': 0.947103274559194, 'political': 0.9355163727959698, 'country': 0.980352644836272, 'rushed': 0.9052896725440807, 'ingredients': 0.9551637279596977, 'side-effect': 0.8473551637279597, 'ineffective': 0.8639798488664987, 'religious': 0.9949622166246851, 'none': 0.9350125944584383}\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
